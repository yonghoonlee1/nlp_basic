{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85758a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file downloaded to fra-eng.zip\n",
      "Extracted files: ['_about.txt', 'fra.txt']\n",
      "샘플 데이터: ('Go.', 'Va !')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "zip_path = \"fra-eng.zip\"\n",
    "txt_path = \"fra.txt\"\n",
    "\n",
    "if not os.path.exists(txt_path):\n",
    "    download_zip(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        print(\"Extracted files:\", zip_ref.namelist())\n",
    "\n",
    "pairs = []\n",
    "with open(txt_path, encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        eng, fra, *_ = line.strip().split(\"\\t\")\n",
    "        pairs.append((eng, fra))\n",
    "\n",
    "pairs = pairs[:50000]\n",
    "\n",
    "print(\"샘플 데이터:\", pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce35bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', 'Va !'),\n",
       " ('Go.', 'Marche.'),\n",
       " ('Go.', 'En route !'),\n",
       " ('Go.', 'Bouge !'),\n",
       " ('Hi.', 'Salut !'),\n",
       " ('Hi.', 'Salut.'),\n",
       " ('Run!', 'Cours\\u202f!'),\n",
       " ('Run!', 'Courez\\u202f!'),\n",
       " ('Run!', 'Prenez vos jambes à vos cous !'),\n",
       " ('Run!', 'File !')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26374344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for txt in texts:\n",
    "        counter.update(tokenize(txt))\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "src_texts = [src for src, _ in pairs]\n",
    "trg_texts = [trg for _, trg in pairs]\n",
    "\n",
    "SRC_VOCAB = build_vocab(src_texts)\n",
    "TRG_VOCAB = build_vocab(trg_texts)\n",
    "\n",
    "PAD_IDX = SRC_VOCAB[\"<pad>\"]\n",
    "SOS_IDX = SRC_VOCAB[\"<sos>\"]\n",
    "EOS_IDX = SRC_VOCAB[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b92683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(text, vocab):\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokenize(text)]\n",
    "\n",
    "def tensor_transform(tokens, vocab):\n",
    "    return torch.tensor([vocab[\"<sos>\"]] + tokens + [vocab[\"<eos>\"]], dtype=torch.long)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, trg_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, trg = self.pairs[idx]\n",
    "        src_tensor = tensor_transform(numericalize(src, self.src_vocab), self.src_vocab)\n",
    "        trg_tensor = tensor_transform(numericalize(trg, self.trg_vocab), self.trg_vocab)\n",
    "        return src_tensor, trg_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "dataset = TranslationDataset(pairs, SRC_VOCAB, TRG_VOCAB)\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_iter = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        hidden, cell = self.encoder(src)\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f4c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_DIM = len(SRC_VOCAB)\n",
    "OUTPUT_DIM = len(TRG_VOCAB)\n",
    "ENC_EMB_DIM, DEC_EMB_DIM = 256, 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85f8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.606\n",
      "Epoch 2, Loss: 3.512\n",
      "Epoch 3, Loss: 2.989\n",
      "Epoch 4, Loss: 2.648\n",
      "Epoch 5, Loss: 2.388\n",
      "Epoch 6, Loss: 2.173\n",
      "Epoch 7, Loss: 2.012\n",
      "Epoch 8, Loss: 1.859\n",
      "Epoch 9, Loss: 1.732\n",
      "Epoch 10, Loss: 1.644\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in iterator:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)  # [trg_len, batch_size, output_dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(model, train_iter, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e072d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = sentence.lower().split()\n",
    "\n",
    "    # 영어 입력을 인덱스로 변환\n",
    "    src_indexes = [src_vocab.get(tok, src_vocab[\"<unk>\"]) for tok in tokens]\n",
    "    src_tensor = torch.tensor([src_vocab[\"<sos>\"]] + src_indexes + [src_vocab[\"<eos>\"]],\n",
    "                              dtype=torch.long).unsqueeze(1).to(device)  # [seq_len, 1]\n",
    "\n",
    "    # 인코더 통과\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # 디코더 초기 입력은 <sos>\n",
    "    trg_indexes = [trg_vocab[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            pred_token = output.argmax(1).item()\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # 인덱스를 단어로 변환\n",
    "    id2word = {idx: word for word, idx in trg_vocab.items()}\n",
    "    trg_tokens = [id2word[idx] for idx in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:]  # <sos> 제외\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f803b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis des <eos>\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"i am hungry\"\n",
    "translation = translate_sentence(model, example_sentence, SRC_VOCAB, TRG_VOCAB, DEVICE)\n",
    "print(\" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6898544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
