{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85758a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 데이터: ('Go.', 'Va !')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"ZIP file downloaded to {output_path}\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n",
    "\n",
    "url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "zip_path = \"fra-eng.zip\"\n",
    "txt_path = \"fra.txt\"\n",
    "\n",
    "if not os.path.exists(txt_path):\n",
    "    download_zip(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "        print(\"Extracted files:\", zip_ref.namelist())\n",
    "\n",
    "pairs = []\n",
    "with open(txt_path, encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        eng, fra, *_ = line.strip().split(\"\\t\")\n",
    "        pairs.append((eng, fra))\n",
    "\n",
    "pairs = pairs[:50000]\n",
    "\n",
    "print(\"샘플 데이터:\", pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce35bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', 'Va !'),\n",
       " ('Go.', 'Marche.'),\n",
       " ('Go.', 'En route !'),\n",
       " ('Go.', 'Bouge !'),\n",
       " ('Hi.', 'Salut !'),\n",
       " ('Hi.', 'Salut.'),\n",
       " ('Run!', 'Cours\\u202f!'),\n",
       " ('Run!', 'Courez\\u202f!'),\n",
       " ('Run!', 'Prenez vos jambes à vos cous !'),\n",
       " ('Run!', 'File !')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26374344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for txt in texts:\n",
    "        counter.update(tokenize(txt))\n",
    "    vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2, \"<unk>\":3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "src_texts = [src for src, _ in pairs]\n",
    "trg_texts = [trg for _, trg in pairs]\n",
    "\n",
    "SRC_VOCAB = build_vocab(src_texts)\n",
    "TRG_VOCAB = build_vocab(trg_texts)\n",
    "\n",
    "PAD_IDX = SRC_VOCAB[\"<pad>\"]\n",
    "SOS_IDX = SRC_VOCAB[\"<sos>\"]\n",
    "EOS_IDX = SRC_VOCAB[\"<eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b92683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(text, vocab):\n",
    "    return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokenize(text)]\n",
    "\n",
    "def tensor_transform(tokens, vocab):\n",
    "    return torch.tensor([vocab[\"<sos>\"]] + tokens + [vocab[\"<eos>\"]], dtype=torch.long)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, trg_vocab):\n",
    "        self.pairs = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, trg = self.pairs[idx]\n",
    "        src_tensor = tensor_transform(numericalize(src, self.src_vocab), self.src_vocab)\n",
    "        trg_tensor = tensor_transform(numericalize(trg, self.trg_vocab), self.trg_vocab)\n",
    "        return src_tensor, trg_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = nn.utils.rnn.pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "dataset = TranslationDataset(pairs, SRC_VOCAB, TRG_VOCAB)\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_iter = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_iter = DataLoader(valid_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f657c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % n_heads == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_k = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.hidden_dim)\n",
    "        x = self.fc_o(x)\n",
    "        return x, attention\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size, src_len = src.shape\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f4c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_DIM = len(SRC_VOCAB)\n",
    "OUTPUT_DIM = len(TRG_VOCAB)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 100\n",
    "\n",
    "enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, DROPOUT, DEVICE, MAX_LEN)\n",
    "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DROPOUT, DEVICE, MAX_LEN)\n",
    "model = Transformer(enc, dec, PAD_IDX, PAD_IDX, DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85f8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.412\n",
      "Epoch 2, Loss: 1.299\n",
      "Epoch 3, Loss: 1.202\n",
      "Epoch 4, Loss: 1.115\n",
      "Epoch 5, Loss: 1.040\n",
      "Epoch 6, Loss: 0.964\n",
      "Epoch 7, Loss: 0.904\n",
      "Epoch 8, Loss: 0.848\n",
      "Epoch 9, Loss: 0.800\n",
      "Epoch 10, Loss: 0.752\n",
      "Epoch 11, Loss: 0.713\n",
      "Epoch 12, Loss: 0.678\n",
      "Epoch 13, Loss: 0.646\n",
      "Epoch 14, Loss: 0.617\n",
      "Epoch 15, Loss: 0.590\n",
      "Epoch 16, Loss: 0.564\n",
      "Epoch 17, Loss: 0.543\n",
      "Epoch 18, Loss: 0.525\n",
      "Epoch 19, Loss: 0.506\n",
      "Epoch 20, Loss: 0.489\n",
      "Epoch 21, Loss: 0.476\n",
      "Epoch 22, Loss: 0.463\n",
      "Epoch 23, Loss: 0.449\n",
      "Epoch 24, Loss: 0.437\n",
      "Epoch 25, Loss: 0.426\n",
      "Epoch 26, Loss: 0.417\n",
      "Epoch 27, Loss: 0.409\n",
      "Epoch 28, Loss: 0.401\n",
      "Epoch 29, Loss: 0.391\n",
      "Epoch 30, Loss: 0.382\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in iterator:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:, :-1])  # teacher forcing\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg_y = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "for epoch in range(30):\n",
    "    loss = train(model, train_iter, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e072d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. 토큰화\n",
    "    tokens = sentence.lower().split()\n",
    "\n",
    "    # 2. 인덱스로 변환\n",
    "    src_indexes = [src_vocab.get(tok, src_vocab[\"<unk>\"]) for tok in tokens]\n",
    "    src_tensor = torch.tensor([src_indexes], dtype=torch.long).to(device)  # [1, src_len]\n",
    "\n",
    "    # 3. Encoder 통과\n",
    "    with torch.no_grad():\n",
    "        src_mask = model.make_src_mask(src_tensor)\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # 4. Decoder 초기 입력: <sos>\n",
    "    trg_indexes = [trg_vocab[\"<sos>\"]]\n",
    "\n",
    "    # 5. step-by-step 디코딩\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indexes], dtype=torch.long).to(device)  # [1, len]\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output[:, -1, :].argmax(1).item()  # 마지막 단어 예측\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # 6. 인덱스를 단어로 변환\n",
    "    id2word = {idx: word for word, idx in trg_vocab.items()}\n",
    "    trg_tokens = [id2word[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:]  # <sos> 제외\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f803b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j'ai faim que je suis faim ! <eos>\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"i am hungry\"\n",
    "translation = translate_sentence(model, example_sentence, SRC_VOCAB, TRG_VOCAB, DEVICE)\n",
    "print(\" \".join(translation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
